{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = './raw_data/sample/.......'\n",
    "sample_path = './raw_data/sample/'\n",
    "all_path = './raw_data/all/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(test_file)\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.findtext('./id_info/nct_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File iterator generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_files(root_path, req_exts='xml'):\n",
    "    \"\"\" builds the generator for all files that match a specified extension in root \"\"\"\n",
    "    # handling a non-list req_ext input\n",
    "    if isinstance(req_exts, str):\n",
    "        req_exts = [req_exts]\n",
    "        \n",
    "    # iterator\n",
    "    for subdirs, dirs, files in os.walk(root_path):\n",
    "        for files in files:\n",
    "            # check the extensions\n",
    "            if req_exts is not None:\n",
    "                cur_ext = file.split('.')[-1]\n",
    "                if cur_ext in req_exts:\n",
    "                    yield os.path.join(sub_dir, file)\n",
    "            else:\n",
    "                yield os.path.join(sub_dir, file)\n",
    "    \n",
    "def list_files(root_path, req_exts='xml'):\n",
    "    \"\"\" wraps the generator to get the list instead \"\"\"\n",
    "    return list(gen_files(root_path, req_exts=req_exts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fps = list_files(sample_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML crawling\n",
    "\n",
    "Methods for obtaining the list of all nodes (or leaves) in the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_tree_tags(root, leaf_only=False):\n",
    "    \"\"\" returns list of all nodes, or leaves \"\"\"\n",
    "    all_nodes = []\n",
    "    for c in root:\n",
    "        all_nodes += crawl_child_tags(c, leaf_only=leaf_only)\n",
    "    return all_nodes\n",
    "\n",
    "def crawl_child_tags(node, leaf_only=False, parent_path='./'):\n",
    "    \"\"\" recursively cralws all children and their children etc ... \"\"\"\n",
    "    all_nodes = []\n",
    "    if not leaf_only or len(code) == 0:\n",
    "        all_nodes += [parent_path + node.tag]\n",
    "        \n",
    "    for c in node:\n",
    "        all_nodes += crawl_child_tags(c, leaf_only=leaf_only, parent_path=parent_path + node.tag + '/')\n",
    "        \n",
    "    return all_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = craw_tree_tags(root, leaf_only=True)\n",
    "len(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = craw_tree_tags(root, leaf_only=False)\n",
    "len(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling files for summary stats and for the node2file dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_files(root_path, leaf_only=True):\n",
    "    \"\"\"\n",
    "    crawls all of the xml files in the roo dir and counts the type of nodes in each\n",
    "    builds the dictionary that helps us go from node name to list of files, \n",
    "    used in further investigating specific nodes\n",
    "    \"\"\"\n",
    "    column_names = ['total_count', 'file_count', 'unique_per_file']\n",
    "    all_fps = list_files(root_path, req_exts='xml')\n",
    "    node_dict = {}  # for building summary dataframe\n",
    "    node2file = {}  # for building the node to file list dictionary\n",
    "    error_files = []\n",
    "    \n",
    "    for fp in tqdm(all_fps):\n",
    "        try:\n",
    "            # list of keys already added by this file\n",
    "            cur_file_nodes = []\n",
    "            \n",
    "            # parsing the xml\n",
    "            tree = ET.parse(fp)\n",
    "            root = tree.getroot()\n",
    "            all_nodes = craw_tree_tags(root, leaf_only=leaf_only)\n",
    "            \n",
    "            # looping throuhg all of the nodes and process both dictionaries\n",
    "            for cur_node in all_nodes:\n",
    "                # --- processing node2file ---\n",
    "                if cur_node not in node2file.keys():\n",
    "                    node2file[cur_node] = [fp]\n",
    "                else:\n",
    "                    # note: duplicate nodes are added multiple times on purpose here\n",
    "                    node2file[cur_node].append(fp)\n",
    "                \n",
    "                # --- processing node_dict ---\n",
    "                if cur_node not in node_dict.keys():\n",
    "                    node_dict[cur_node] = {column_names[0]: 0, \n",
    "                                           column_names[1]: 0,\n",
    "                                           column_names[2]: True}\n",
    "                # increment the counters\n",
    "                if cur_node in cur_file_nodes:\n",
    "                    # not the first occurance of this node in this file\n",
    "                    node_dict[cur_node][column_names[0]] += 1\n",
    "                    node_dict[cur_node][columns[2]] = False  # not unique\n",
    "                else:\n",
    "                    # first occurance of this node in this file\n",
    "                    cur_file_nodes.append(cur_node)\n",
    "                    node_dict[cur_node][column_names[0]] += 1\n",
    "                    node_dict[cur_node][column_names[1]] += 1\n",
    "        except Exception:\n",
    "            error_files.append(fp)\n",
    "            pass\n",
    "        \n",
    "    df = pd.DataFrame(node_dict, index=column_names).T\n",
    "    \n",
    "    # some convienence calcs on the final dataframe\n",
    "    df.index.name = 'node'\n",
    "    df['level'] = df.reset_index()['node'].apply()(lambda x: len(x.split('/')) - 1).values\n",
    "    df['avg_per_file'] = df['total_count'] / df['file_count']\n",
    "    df['pct_files'] = df['file_count'] / len(all_fps)\n",
    "    \n",
    "    return df, node2file, error_files\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample, node2file_sample, didnotcrawl = crawl_files(sample_path, leaf_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.to_csv('res_sample.csv')\n",
    "pickle.dump(node2file_sample, open('node2file_sample.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, node2file, didnotcrawl = crawl_files(all_path, leaf_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('res.csv')\n",
    "pickle.dump(node2file, open('node2file.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some plots of stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(df['pct_files'].sort_values(ascending=False).values, label='coverage')\n",
    "plt.title('Fields sorted by % coverage')\n",
    "plt.xlabel('field number')\n",
    "plt.ylabel('pct coverage')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('coverage by XML depth')\n",
    "for cur_lvl in sorted(df['level'].unique()):\n",
    "    sub_df = df[df['level'] == cur_lvl]\n",
    "    plt.plot(sub_df['pct_files'].sort_values(ascending=False).values, label='depth='+str(cur_lvl))\n",
    "plt.legend()\n",
    "plt.xlabel('field number')\n",
    "plt.ylabel('pct coverage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Usage?\n",
    "tot_size = sys.getsizeof(node2file)\n",
    "for k, v in node2file.items():\n",
    "    tot_size += sys.getsizeof(v)\n",
    "    \n",
    "tot_size / 1024 / 1024  # in MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Data Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_node_text(node, node2file, n=10, apply_fn=None, include_fn=False):\n",
    "    \"\"\"\n",
    "    provided a node string and a node2file map, sample the texts in the raw files\n",
    "    returns either a list or a dict depending on the include_fn parameter\n",
    "    :param node: string rep of a node\n",
    "    :param node2file: pre-crawled dict of node string -> files that contain the node\n",
    "    :param n: number of samples to return\n",
    "    :param apply_fn: applys a function to each sampled text\n",
    "    :param include_fn: if true, returns a dict of filename -> field text instead\n",
    "    \"\"\"\n",
    "    all_files = node2file[node]\n",
    "    tot_files = len(all_files)\n",
    "    \n",
    "    if tot_files < n:\n",
    "        print(\"only %s files with this node were found, returning all of them\" % tot_files)\n",
    "        n = tot_files\n",
    "        \n",
    "    chosen_files = random.choices(all_files, k=n)\n",
    "    if include_fn:\n",
    "        return dict(zip(chosen_files, get_field_from_files(node, chosen_files, apply_fn)))\n",
    "    else:\n",
    "        return get_field_from_files(node, chosen_files, apply_fn)\n",
    "    \n",
    "def get_field_from_files(node, chosen_files, apply_fn=None):\n",
    "    \"\"\"\n",
    "    loads each file and attempts to extract the node from that file, returns the text from that file\n",
    "    with an optional apply_fn to apply to the text\n",
    "    :param node: string representation of a node\n",
    "    :param list_files: list of file locations\n",
    "    :param apply_fn: applys a transformation function to each sampled text\n",
    "    \"\"\"\n",
    "    rt_arr = []\n",
    "    for cur_file in list_files:\n",
    "        raw_text = extract_text_from_file(node, cur_file)\n",
    "        if apply_fn is None:\n",
    "            rt_arr.append(raw_text)\n",
    "        else:\n",
    "            rt_arr.append(apply_fn(raw_text))\n",
    "    return rt_arr\n",
    "\n",
    "def extract_text_from_file(node, cur_file):\n",
    "    \"\"\"\n",
    "    attempts to extract the text from a node, otherwise returns an error representation string\n",
    "    :param node: string representation of a node\n",
    "    :param cur_file: string path of the file to load\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(cur_file)\n",
    "        root = tree.getroot()\n",
    "        raw_text = root.findtext(node)\n",
    "        return raw_text\n",
    "    except Exception:\n",
    "        print(\"ERROR: node %s was not found in file %s\" % (node, cur_file))\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_node = './clincal_results/outcome_list/outcome/group_list/group/title'\n",
    "sample_text = sample_node_text(cur_node, node2file, 5, include_fn=True)\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "n_samples = 5\n",
    "\n",
    "for cur_node in tqdm(node2file.keys()):\n",
    "    sample_text = sample_node_text(cur_node, node2file, n_samples)\n",
    "    res_dict[cur_node] = dict(zip(['ex' + str(x + 1) for x in range(0, len(sample_text))], sample_text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(res_dict).T\n",
    "df_res.index.name = 'node'\n",
    "df_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
